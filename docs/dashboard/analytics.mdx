---
title: "Analytics"
description: "Track token usage, model breakdown, cost trends, and more"
---

# Analytics

The Analytics page aggregates data across all your sessions to show usage patterns, costs, and trends. All metrics are computed in real time from your session data.

## Summary metrics

The top of the page shows four headline numbers:

| Metric | Calculation | Description |
| --- | --- | --- |
| Total Sessions | Count of sessions | How many coding sessions you have synced |
| Total Tokens | Sum of `totalTokens` across all sessions | Combined prompt and completion token count |
| Total Cost | Sum of `cost` across all sessions | Estimated USD spend based on model pricing |
| Avg Tokens/Session | Total tokens / total sessions | Average token consumption per session |

## Usage by source

A bar chart breaks down sessions, tokens, and cost by source tool:

| Source | Sessions | Tokens | Cost |
| --- | --- | --- | --- |
| opencode | Real count | Real sum | Real sum |
| claude-code | Real count | Real sum | Real sum |
| codex-cli | Real count | Real sum | Real sum |
| cursor | Real count | Real sum | Real sum |

The `source` field is set by each sync plugin. It identifies which tool produced the session. Only sources with at least one session appear in the chart.

This breakdown helps you understand which tools you use most and where your spending concentrates.

## Usage by model

A separate chart shows the same metrics grouped by AI model:

| Model | Sessions | Tokens | Cost |
| --- | --- | --- | --- |
| claude-sonnet-4-20250514 | - | - | - |
| gpt-4o | - | - | - |
| o3-mini | - | - | - |
| gpt-4o-mini | - | - | - |

The `model` field comes from the session metadata synced by each plugin. Model names match whatever the provider returns (e.g., "claude-sonnet-4-20250514", "gpt-4o-2024-08-06").

## Cost calculation

OpenSync estimates costs using published pricing for common models. The calculation happens in the sync plugin at the time of session creation:

```
cost = (promptTokens / 1,000,000 * inputPricePerMillion)
     + (completionTokens / 1,000,000 * outputPricePerMillion)
```

Current pricing used:

| Model | Input (per 1M tokens) | Cached Input | Output (per 1M tokens) |
| --- | --- | --- | --- |
| claude-sonnet-4-20250514 | $3.00 | $1.50 | $15.00 |
| claude-3-5-haiku | $1.00 | $0.50 | $5.00 |
| gpt-4o | $2.50 | $1.25 | $10.00 |
| gpt-4o-mini | $0.15 | $0.075 | $0.60 |
| o3-mini | $1.10 | $0.55 | $4.40 |
| o1-mini | $1.10 | $0.55 | $4.40 |

<Info title="Cost accuracy">
  Cost estimates are approximations based on published list prices. Actual costs depend on your provider agreements, commitment discounts, and cached token rates. Use these numbers for relative comparison rather than exact billing.
</Info>

## Token breakdown

Each session tracks tokens in two categories:

- **Prompt tokens** (`promptTokens`): Tokens sent to the model as input. This includes your message, system prompts, conversation history, and any context the tool injects.
- **Completion tokens** (`completionTokens`): Tokens generated by the model in response. This includes the text reply and any tool call arguments.

The Analytics page shows the prompt-to-completion ratio, which gives insight into how much context your tools inject versus how much the model generates.

## Time range filter

All analytics respect the time range selector:

| Preset | Filter |
| --- | --- |
| Today | Sessions created since midnight (local time) |
| Last 7 days | Sessions from the past 7 calendar days |
| Last 30 days | Sessions from the past 30 calendar days |
| All time | No date filter applied |

Changing the time range recalculates all metrics, charts, and tables in real time.

## Trend charts

Below the summary cards, trend charts show:

- **Sessions per day** over the selected time range
- **Tokens per day** split by prompt and completion
- **Cost per day** as a stacked bar chart by model

These charts help you spot usage spikes, identify expensive sessions, and track whether your costs are increasing or decreasing.

## Project breakdown

If your sessions include project metadata (the `projectName` field), the Analytics page shows a table grouped by project:

| Project | Sessions | Tokens | Cost | Top model |
| --- | --- | --- | --- | --- |
| opensync | 12 | 45,000 | $0.89 | claude-sonnet-4-20250514 |
| my-app | 8 | 23,000 | $0.42 | gpt-4o |

This helps you track which projects consume the most AI resources.

## Next steps

<CardGroup cols={2}>
  <Card title="Sessions View" href="/dashboard/sessions" description="Drill into individual sessions" />
  <Card title="Evals Export" href="/dashboard/evals" description="Label and export sessions for evaluation" />
  <Card title="API Reference" href="/api/endpoints" description="Query analytics data programmatically" />
</CardGroup>
